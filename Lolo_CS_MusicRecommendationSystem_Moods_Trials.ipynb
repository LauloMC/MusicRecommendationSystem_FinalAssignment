{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoderSchool Final Project Moods\n",
    "## Music Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>audio_features</th>\n",
       "      <th>context</th>\n",
       "      <th>decades</th>\n",
       "      <th>genres</th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "      <th>name</th>\n",
       "      <th>new_context</th>\n",
       "      <th>picture</th>\n",
       "      <th>recording_id</th>\n",
       "      <th>sub_context</th>\n",
       "      <th>yt_id</th>\n",
       "      <th>yt_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'$oid': '52fdfb440b9398049f3d7a8c'}</td>\n",
       "      <td>Gangnam Style (강남스타일)</td>\n",
       "      <td>PSY</td>\n",
       "      <td>[11, 0.912744, 0.083704, 132.069, 0.293137, 0....</td>\n",
       "      <td>[work out]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[pop]</td>\n",
       "      <td>[oppa, gangnam, style, gangnam, style, najeneu...</td>\n",
       "      <td>[energetic, motivational]</td>\n",
       "      <td>Gangnam Style (강남스타일)</td>\n",
       "      <td>work out</td>\n",
       "      <td>http://images.musicnet.com/albums/073/463/405/...</td>\n",
       "      <td>50232.0</td>\n",
       "      <td>[working out: cardio]</td>\n",
       "      <td>9bZkp7q19f0</td>\n",
       "      <td>2450112089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'$oid': '52fdfb3d0b9398049f3cbc8e'}</td>\n",
       "      <td>Native</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>[6, 0.7457039999999999, 0.11995499999999999, 1...</td>\n",
       "      <td>[energetic]</td>\n",
       "      <td>[2012]</td>\n",
       "      <td>[pop]</td>\n",
       "      <td>[lately, i, ve, been, i, ve, been, losing, sle...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>Counting Stars</td>\n",
       "      <td>energetic</td>\n",
       "      <td>http://images.musicnet.com/albums/081/851/887/...</td>\n",
       "      <td>5839.0</td>\n",
       "      <td>[energy boost]</td>\n",
       "      <td>hT_nvWreIhg</td>\n",
       "      <td>1020297206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'$oid': '52fdfb420b9398049f3d3ea5'}</td>\n",
       "      <td>Party Rock Anthem</td>\n",
       "      <td>LMFAO</td>\n",
       "      <td>[5, 0.709932, 0.231455, 130.03, 0.121740999999...</td>\n",
       "      <td>[energetic, energetic, energetic, energetic]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[party, rock, yeah, woo, let, s, go, party, ro...</td>\n",
       "      <td>[happy, celebratory, rowdy]</td>\n",
       "      <td>Party Rock Anthem</td>\n",
       "      <td>housework</td>\n",
       "      <td>http://images.musicnet.com/albums/049/414/127/...</td>\n",
       "      <td>52379.0</td>\n",
       "      <td>[energy boost, pleasing a crowd, housework, dr...</td>\n",
       "      <td>KQ6zr6kCPj8</td>\n",
       "      <td>971128436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id                  album       artist  \\\n",
       "0  {'$oid': '52fdfb440b9398049f3d7a8c'}  Gangnam Style (강남스타일)          PSY   \n",
       "1  {'$oid': '52fdfb3d0b9398049f3cbc8e'}                 Native  OneRepublic   \n",
       "2  {'$oid': '52fdfb420b9398049f3d3ea5'}      Party Rock Anthem        LMFAO   \n",
       "\n",
       "                                      audio_features  \\\n",
       "0  [11, 0.912744, 0.083704, 132.069, 0.293137, 0....   \n",
       "1  [6, 0.7457039999999999, 0.11995499999999999, 1...   \n",
       "2  [5, 0.709932, 0.231455, 130.03, 0.121740999999...   \n",
       "\n",
       "                                        context decades genres  \\\n",
       "0                                    [work out]      []  [pop]   \n",
       "1                                   [energetic]  [2012]  [pop]   \n",
       "2  [energetic, energetic, energetic, energetic]      []     []   \n",
       "\n",
       "                                     lyrics_features  \\\n",
       "0  [oppa, gangnam, style, gangnam, style, najeneu...   \n",
       "1  [lately, i, ve, been, i, ve, been, losing, sle...   \n",
       "2  [party, rock, yeah, woo, let, s, go, party, ro...   \n",
       "\n",
       "                         moods                   name new_context  \\\n",
       "0    [energetic, motivational]  Gangnam Style (강남스타일)    work out   \n",
       "1                      [happy]         Counting Stars   energetic   \n",
       "2  [happy, celebratory, rowdy]      Party Rock Anthem   housework   \n",
       "\n",
       "                                             picture  recording_id  \\\n",
       "0  http://images.musicnet.com/albums/073/463/405/...       50232.0   \n",
       "1  http://images.musicnet.com/albums/081/851/887/...        5839.0   \n",
       "2  http://images.musicnet.com/albums/049/414/127/...       52379.0   \n",
       "\n",
       "                                         sub_context        yt_id    yt_views  \n",
       "0                              [working out: cardio]  9bZkp7q19f0  2450112089  \n",
       "1                                     [energy boost]  hT_nvWreIhg  1020297206  \n",
       "2  [energy boost, pleasing a crowd, housework, dr...  KQ6zr6kCPj8   971128436  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_json('MasterSongList.json')\n",
    "full_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[oppa, gangnam, style, gangnam, style, najeneu...</td>\n",
       "      <td>[energetic, motivational]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[lately, i, ve, been, i, ve, been, losing, sle...</td>\n",
       "      <td>[happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[party, rock, yeah, woo, let, s, go, party, ro...</td>\n",
       "      <td>[happy, celebratory, rowdy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[alagamun, lan, weh, wakun, heya, hanun, gon, ...</td>\n",
       "      <td>[happy, energetic, celebratory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[j, lo, the, other, side, out, my, mine, it, s...</td>\n",
       "      <td>[energetic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     lyrics_features  \\\n",
       "0  [oppa, gangnam, style, gangnam, style, najeneu...   \n",
       "1  [lately, i, ve, been, i, ve, been, losing, sle...   \n",
       "2  [party, rock, yeah, woo, let, s, go, party, ro...   \n",
       "3  [alagamun, lan, weh, wakun, heya, hanun, gon, ...   \n",
       "4  [j, lo, the, other, side, out, my, mine, it, s...   \n",
       "\n",
       "                             moods  \n",
       "0        [energetic, motivational]  \n",
       "1                          [happy]  \n",
       "2      [happy, celebratory, rowdy]  \n",
       "3  [happy, energetic, celebratory]  \n",
       "4                      [energetic]  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['lyrics_features', 'moods']\n",
    "lyrics = full_df.copy()\n",
    "lyrics = lyrics[cols]\n",
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the wrong format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics_features</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oppa gangnam style gangnam style najeneun ttas...</td>\n",
       "      <td>[energetic, motivational]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lately i ve been i ve been losing sleep dreami...</td>\n",
       "      <td>[happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>party rock yeah woo let s go party rock is in ...</td>\n",
       "      <td>[happy, celebratory, rowdy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alagamun lan weh wakun heya hanun gon alagamun...</td>\n",
       "      <td>[happy, energetic, celebratory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j lo the other side out my mine it s a new gen...</td>\n",
       "      <td>[energetic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     lyrics_features  \\\n",
       "0  oppa gangnam style gangnam style najeneun ttas...   \n",
       "1  lately i ve been i ve been losing sleep dreami...   \n",
       "2  party rock yeah woo let s go party rock is in ...   \n",
       "3  alagamun lan weh wakun heya hanun gon alagamun...   \n",
       "4  j lo the other side out my mine it s a new gen...   \n",
       "\n",
       "                             moods  \n",
       "0        [energetic, motivational]  \n",
       "1                          [happy]  \n",
       "2      [happy, celebratory, rowdy]  \n",
       "3  [happy, energetic, celebratory]  \n",
       "4                      [energetic]  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics['lyrics_features'] = lyrics['lyrics_features'].apply(' '.join)\n",
    "# lyrics['moods'] = lyrics['moods'].apply(', '.join)\n",
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace empty lyrics with NaN and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36733, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics['lyrics_features'].replace('', np.nan, inplace=True)\n",
    "lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20931, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.dropna(subset=['lyrics_features'], inplace=True)\n",
    "lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20931, 2)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics['moods'] = lyrics['moods'].apply(lambda y: np.nan if len(y)==0 else y)\n",
    "lyrics.dropna(subset=['moods'], inplace=True)\n",
    "lyrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moods cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aggressive',\n",
       " 'angsty',\n",
       " 'atmospheric',\n",
       " 'campy',\n",
       " 'celebratory',\n",
       " 'classy',\n",
       " 'cocky',\n",
       " 'cold',\n",
       " 'earthy',\n",
       " 'energetic',\n",
       " 'funky',\n",
       " 'gloomy',\n",
       " 'happy',\n",
       " 'hypnotic',\n",
       " 'introspective',\n",
       " 'lush',\n",
       " 'mellow',\n",
       " 'motivational',\n",
       " 'nocturnal',\n",
       " 'raw',\n",
       " 'rowdy',\n",
       " 'sad',\n",
       " 'seductive',\n",
       " 'sexual',\n",
       " 'soothing',\n",
       " 'spacey',\n",
       " 'sprightly',\n",
       " 'sweet',\n",
       " 'trashy',\n",
       " 'trippy',\n",
       " 'visceral',\n",
       " 'warm'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moods_list = lyrics['moods'].tolist()\n",
    "moods_set = set(x for i in moods_list for x in i)\n",
    "moods_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moods distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nocturnal :  1334\n",
      "sweet :  814\n",
      "sprightly :  1733\n",
      "celebratory :  1479\n",
      "cold :  830\n",
      "campy :  636\n",
      "energetic :  2305\n",
      "visceral :  1112\n",
      "earthy :  873\n",
      "funky :  2072\n",
      "classy :  492\n",
      "seductive :  1419\n",
      "soothing :  1374\n",
      "trashy :  477\n",
      "cocky :  1438\n",
      "raw :  1301\n",
      "sexual :  505\n",
      "warm :  1495\n",
      "rowdy :  1721\n",
      "spacey :  514\n",
      "mellow :  2856\n",
      "lush :  1326\n",
      "happy :  1757\n",
      "trippy :  780\n",
      "motivational :  925\n",
      "aggressive :  1683\n",
      "gloomy :  750\n",
      "angsty :  1205\n",
      "atmospheric :  1155\n",
      "sad :  1249\n",
      "introspective :  1417\n",
      "hypnotic :  286\n"
     ]
    }
   ],
   "source": [
    "def number_moods(mood):\n",
    "    counter=0\n",
    "    for i in lyrics['moods']:\n",
    "        if mood in i:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "for i in moods_set:\n",
    "    mood_count = number_moods(i)\n",
    "    print(i, \": \", mood_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the moods are more balanced than the genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Lyrics cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    from string import punctuation\n",
    "    from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "    # Create empty list to receive result\n",
    "    clean_words = []\n",
    "    \n",
    "    # 1. Convert to lower case\n",
    "    raw_text = raw_text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    raw_text = raw_text.translate(translator)\n",
    "    split_words = raw_text.split()\n",
    "    \n",
    "    # 3 & 4. Remove common words and stem words\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    for word in split_words:\n",
    "        if word not in ENGLISH_STOP_WORDS:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            clean_words.append(stemmed_word)\n",
    "            \n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clean_text, open('clean_text_function.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics['clean_lyrics'] = lyrics['lyrics_features'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lyrics</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oppa gangnam style gangnam style najeneun ttas...</td>\n",
       "      <td>[energetic, motivational]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>late ve ve lose sleep dream thing babi ve ve p...</td>\n",
       "      <td>[happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parti rock yeah woo let s parti rock hous toni...</td>\n",
       "      <td>[happy, celebratory, rowdy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alagamun lan weh wakun heya hanun gon alagamun...</td>\n",
       "      <td>[happy, energetic, celebratory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j lo s new generat mr worldwid parti peopl flo...</td>\n",
       "      <td>[energetic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_lyrics  \\\n",
       "0  oppa gangnam style gangnam style najeneun ttas...   \n",
       "1  late ve ve lose sleep dream thing babi ve ve p...   \n",
       "2  parti rock yeah woo let s parti rock hous toni...   \n",
       "3  alagamun lan weh wakun heya hanun gon alagamun...   \n",
       "4  j lo s new generat mr worldwid parti peopl flo...   \n",
       "\n",
       "                             moods  \n",
       "0        [energetic, motivational]  \n",
       "1                          [happy]  \n",
       "2      [happy, celebratory, rowdy]  \n",
       "3  [happy, energetic, celebratory]  \n",
       "4                      [energetic]  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols2 = ['clean_lyrics', 'moods']\n",
    "new_lyrics = lyrics.copy()\n",
    "new_lyrics = new_lyrics[cols2]\n",
    "new_lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Try classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the following:\n",
    "- TF-IDF with MultiLabelBinarizer and a Classifier Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 44060)\t0.22652096467632132\n",
      "  (0, 24533)\t0.3523880733352704\n",
      "  (0, 58510)\t0.15339492137542518\n",
      "  (0, 41403)\t0.04145742039238475\n",
      "  (0, 62767)\t0.04145742039238475\n",
      "  (0, 30492)\t0.020728710196192376\n",
      "  (0, 67851)\t0.1510139764508809\n",
      "  (0, 33381)\t0.04145742039238475\n",
      "  (0, 27396)\t0.020728710196192376\n",
      "  (0, 67889)\t0.020728710196192376\n",
      "  (0, 2942)\t0.05663024116908033\n",
      "  (0, 48986)\t0.020728710196192376\n",
      "  (0, 30595)\t0.03981840690336137\n",
      "  (0, 5433)\t0.03865550832622193\n",
      "  (0, 43796)\t0.04145742039238475\n",
      "  (0, 55709)\t0.03981840690336137\n",
      "  (0, 62793)\t0.020728710196192376\n",
      "  (0, 25185)\t0.09954601725840342\n",
      "  (0, 5521)\t0.020728710196192376\n",
      "  (0, 41540)\t0.10756078878017733\n",
      "  (0, 53233)\t0.20728710196192376\n",
      "  (0, 42050)\t0.020728710196192376\n",
      "  (0, 55628)\t0.020728710196192376\n",
      "  (0, 31908)\t0.018196685414216394\n",
      "  (0, 67077)\t0.020728710196192376\n",
      "  :\t:\n",
      "  (0, 40712)\t0.020728710196192376\n",
      "  (0, 38863)\t0.018876747056360114\n",
      "  (0, 49017)\t0.020728710196192376\n",
      "  (0, 24662)\t0.020728710196192376\n",
      "  (0, 66369)\t0.020728710196192376\n",
      "  (0, 42618)\t0.020728710196192376\n",
      "  (0, 67575)\t0.01768874067408758\n",
      "  (0, 24479)\t0.020728710196192376\n",
      "  (0, 31905)\t0.020728710196192376\n",
      "  (0, 62746)\t0.020728710196192376\n",
      "  (0, 17762)\t0.020728710196192376\n",
      "  (0, 65993)\t0.019909203451680686\n",
      "  (0, 39177)\t0.020728710196192376\n",
      "  (0, 25171)\t0.020728710196192376\n",
      "  (0, 53465)\t0.020728710196192376\n",
      "  (0, 63428)\t0.020728710196192376\n",
      "  (0, 62810)\t0.03865550832622193\n",
      "  (0, 42683)\t0.09772670784603703\n",
      "  (0, 25128)\t0.03331256855753402\n",
      "  (0, 66721)\t0.03537748134817516\n",
      "  (0, 4912)\t0.020389261133688462\n",
      "  (0, 41017)\t0.03701649483719855\n",
      "  (0, 32375)\t0.03305168419389629\n",
      "  (0, 33952)\t0.0035161875688652536\n",
      "  (0, 53625)\t0.004463790973120557\n"
     ]
    }
   ],
   "source": [
    "tf_idf = vectorizer.fit_transform(new_lyrics['clean_lyrics'])\n",
    "print(tf_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tf_idf\n",
    "y = new_lyrics['moods']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the multilabel binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_bina = mlb.fit_transform(y)\n",
    "y_bina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mlb, open('mlb.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bina, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC1 TFIDF with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_rfc1 = ClassifierChain(RandomForestClassifier())\n",
    "chain_rfc1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.03      0.05       195\n",
      "          1       0.00      0.00      0.00       118\n",
      "          2       0.00      0.00      0.00       118\n",
      "          3       0.00      0.00      0.00        63\n",
      "          4       0.38      0.03      0.06       151\n",
      "          5       0.50      0.07      0.12        45\n",
      "          6       0.33      0.01      0.02       162\n",
      "          7       0.17      0.01      0.02        88\n",
      "          8       0.00      0.00      0.00        91\n",
      "          9       0.37      0.05      0.10       201\n",
      "         10       0.36      0.04      0.08       187\n",
      "         11       0.00      0.00      0.00        75\n",
      "         12       0.09      0.01      0.01       196\n",
      "         13       0.00      0.00      0.00        20\n",
      "         14       0.00      0.00      0.00       122\n",
      "         15       0.17      0.01      0.01       144\n",
      "         16       0.35      0.03      0.05       267\n",
      "         17       0.00      0.00      0.00        95\n",
      "         18       0.22      0.01      0.03       140\n",
      "         19       0.17      0.01      0.01       145\n",
      "         20       0.25      0.02      0.04       177\n",
      "         21       0.00      0.00      0.00       121\n",
      "         22       0.36      0.03      0.05       135\n",
      "         23       0.00      0.00      0.00        54\n",
      "         24       0.40      0.01      0.03       145\n",
      "         25       0.00      0.00      0.00        53\n",
      "         26       0.29      0.03      0.05       149\n",
      "         27       0.67      0.05      0.09        81\n",
      "         28       0.00      0.00      0.00        45\n",
      "         29       0.10      0.04      0.06        75\n",
      "         30       0.29      0.02      0.04        98\n",
      "         31       0.00      0.00      0.00       154\n",
      "\n",
      "avg / total       0.23      0.02      0.03      3910\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_rfc1 = chain_rfc1.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rfc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooooh what a really bad score, let's try something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### RFC2 TFIDF without Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 100], 'min_samples_split': [2, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2 = RandomForestClassifier()\n",
    "parameters_rfc2 = {'n_estimators':[5, 10, 100], 'min_samples_split':[2, 5, 10], 'max_features':['sqrt', 'log2', 'auto']}\n",
    "grid_rfc2 = GridSearchCV(rfc2, parameters_rfc2)\n",
    "grid_rfc2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.05      0.09       195\n",
      "          1       0.00      0.00      0.00       118\n",
      "          2       0.00      0.00      0.00       118\n",
      "          3       0.00      0.00      0.00        63\n",
      "          4       0.20      0.03      0.05       151\n",
      "          5       0.33      0.07      0.11        45\n",
      "          6       0.24      0.03      0.05       162\n",
      "          7       0.17      0.01      0.02        88\n",
      "          8       0.00      0.00      0.00        91\n",
      "          9       0.32      0.06      0.10       201\n",
      "         10       0.32      0.10      0.15       187\n",
      "         11       0.00      0.00      0.00        75\n",
      "         12       0.11      0.01      0.02       196\n",
      "         13       0.00      0.00      0.00        20\n",
      "         14       0.11      0.01      0.02       122\n",
      "         15       0.08      0.01      0.01       144\n",
      "         16       0.31      0.06      0.10       267\n",
      "         17       0.00      0.00      0.00        95\n",
      "         18       0.22      0.03      0.05       140\n",
      "         19       0.08      0.01      0.01       145\n",
      "         20       0.19      0.02      0.03       177\n",
      "         21       0.00      0.00      0.00       121\n",
      "         22       0.31      0.07      0.12       135\n",
      "         23       0.00      0.00      0.00        54\n",
      "         24       0.18      0.02      0.04       145\n",
      "         25       0.33      0.04      0.07        53\n",
      "         26       0.13      0.03      0.04       149\n",
      "         27       0.67      0.05      0.09        81\n",
      "         28       0.00      0.00      0.00        45\n",
      "         29       0.14      0.01      0.02        75\n",
      "         30       0.15      0.04      0.06        98\n",
      "         31       0.12      0.01      0.02       154\n",
      "\n",
      "avg / total       0.18      0.03      0.05      3910\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(grid_rfc2.best_estimator_)\n",
    "predictions_rfc2 = grid_rfc2.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rfc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is even worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the 2 same things but with kNN instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN1 TFIDF without classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] n_neighbors=3 ...................................................\n",
      "[CV] .......................... n_neighbors=3, score=0.019271 -   6.8s\n",
      "[CV] n_neighbors=3 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......................... n_neighbors=3, score=0.022774 -   4.8s\n",
      "[CV] n_neighbors=3 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   11.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .......................... n_neighbors=3, score=0.022615 -   4.7s\n",
      "[CV] n_neighbors=6 ...................................................\n",
      "[CV] .......................... n_neighbors=6, score=0.007645 -   5.3s\n",
      "[CV] n_neighbors=6 ...................................................\n",
      "[CV] .......................... n_neighbors=6, score=0.007804 -   5.4s\n",
      "[CV] n_neighbors=6 ...................................................\n",
      "[CV] .......................... n_neighbors=6, score=0.008759 -   5.3s\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] ......................... n_neighbors=10, score=0.007963 -   5.3s\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] ......................... n_neighbors=10, score=0.004778 -   5.3s\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] ......................... n_neighbors=10, score=0.006848 -   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   48.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [3, 6, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier()\n",
    "parameters_knn1 = {'n_neighbors':[3,6,10]}\n",
    "grid_knn1 = GridSearchCV(knn1, parameters_knn1, verbose=3)\n",
    "grid_knn1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.23      0.21       195\n",
      "          1       0.00      0.00      0.00       118\n",
      "          2       0.00      0.00      0.00       118\n",
      "          3       0.25      0.02      0.03        63\n",
      "          4       0.24      0.05      0.09       151\n",
      "          5       0.67      0.09      0.16        45\n",
      "          6       0.40      0.07      0.12       162\n",
      "          7       0.20      0.02      0.04        88\n",
      "          8       0.00      0.00      0.00        91\n",
      "          9       0.22      0.06      0.09       201\n",
      "         10       0.25      0.10      0.15       187\n",
      "         11       0.00      0.00      0.00        75\n",
      "         12       0.17      0.03      0.05       196\n",
      "         13       0.00      0.00      0.00        20\n",
      "         14       0.27      0.03      0.06       122\n",
      "         15       0.06      0.01      0.01       144\n",
      "         16       0.22      0.05      0.08       267\n",
      "         17       0.18      0.02      0.04        95\n",
      "         18       0.17      0.05      0.08       140\n",
      "         19       0.08      0.01      0.01       145\n",
      "         20       0.17      0.03      0.05       177\n",
      "         21       0.25      0.02      0.05       121\n",
      "         22       0.15      0.04      0.07       135\n",
      "         23       0.29      0.04      0.07        54\n",
      "         24       0.22      0.01      0.03       145\n",
      "         25       0.00      0.00      0.00        53\n",
      "         26       0.30      0.04      0.07       149\n",
      "         27       0.20      0.02      0.04        81\n",
      "         28       0.25      0.02      0.04        45\n",
      "         29       0.00      0.00      0.00        75\n",
      "         30       0.07      0.11      0.09        98\n",
      "         31       0.04      0.01      0.01       154\n",
      "\n",
      "avg / total       0.18      0.05      0.06      3910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grid_knn1.best_estimator_)\n",
    "predictions_knn1 = grid_knn1.predict(X_test)\n",
    "print(classification_report(y_test, predictions_knn1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN2 TFIDF with classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_knn2 = ClassifierChain(KNeighborsClassifier())\n",
    "chain_knn2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.09      0.13       195\n",
      "          1       0.00      0.00      0.00       118\n",
      "          2       0.25      0.01      0.02       118\n",
      "          3       0.33      0.02      0.03        63\n",
      "          4       0.22      0.05      0.08       151\n",
      "          5       0.56      0.11      0.19        45\n",
      "          6       0.37      0.06      0.11       162\n",
      "          7       0.25      0.01      0.02        88\n",
      "          8       0.14      0.01      0.02        91\n",
      "          9       0.35      0.07      0.12       201\n",
      "         10       0.20      0.14      0.17       187\n",
      "         11       0.50      0.01      0.03        75\n",
      "         12       0.10      0.02      0.03       196\n",
      "         13       0.00      0.00      0.00        20\n",
      "         14       0.06      0.01      0.01       122\n",
      "         15       0.10      0.01      0.02       144\n",
      "         16       0.15      0.06      0.08       267\n",
      "         17       0.12      0.03      0.05        95\n",
      "         18       0.13      0.07      0.09       140\n",
      "         19       0.13      0.03      0.05       145\n",
      "         20       0.23      0.12      0.16       177\n",
      "         21       0.16      0.02      0.04       121\n",
      "         22       0.13      0.13      0.13       135\n",
      "         23       0.17      0.09      0.12        54\n",
      "         24       0.07      0.03      0.04       145\n",
      "         25       1.00      0.02      0.04        53\n",
      "         26       0.10      0.32      0.15       149\n",
      "         27       0.10      0.19      0.13        81\n",
      "         28       0.17      0.04      0.07        45\n",
      "         29       0.04      0.12      0.06        75\n",
      "         30       0.16      0.06      0.09        98\n",
      "         31       0.08      0.25      0.12       154\n",
      "\n",
      "avg / total       0.19      0.08      0.08      3910\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_knn2 = chain_knn2.predict(X_test)\n",
    "print(classification_report(y_test, predictions_knn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a very low score, let's first do a cross-validation on the one with the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_rfc1 = cross_val_score(chain_rfc1, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0065297  0.00589266 0.00844083]\n",
      "0.006954398258746085\n"
     ]
    }
   ],
   "source": [
    "print(scores_rfc1)\n",
    "print(scores_rfc1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not helpful :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at different data from TF-IDF:\n",
    "- remove words with frequency lower than 10\n",
    "- use bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 22624)\t0.17555162985564912\n",
      "  (0, 16653)\t0.1396707381704529\n",
      "  (0, 10216)\t0.06283741917512049\n",
      "  (0, 7993)\t0.027503072721477898\n",
      "  (0, 17515)\t0.78313228579269\n",
      "  (0, 6175)\t0.4054680866662382\n",
      "  (0, 20937)\t0.07595136264876994\n",
      "  (0, 12341)\t0.05672612816713428\n",
      "  (0, 17042)\t0.11184257398463758\n",
      "  (0, 8170)\t0.03812431110832099\n",
      "  (0, 1081)\t0.0233343320070627\n",
      "  (0, 11285)\t0.03782574401868204\n",
      "  (0, 11891)\t0.00402407363332277\n",
      "  (0, 20459)\t0.00510855101094494\n",
      "  (0, 20938)\t0.10784996317178018\n",
      "  (0, 6176)\t0.3605558962184408\n",
      "  (0, 1088)\t0.022389745369421107\n",
      "  (0, 12138)\t0.012979907856845323\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf_idf2 = vectorizer2.fit_transform(new_lyrics['clean_lyrics'])\n",
    "print(tf_idf2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer2, open('tfidf_vectorizer2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf2 = tf_idf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try this data on our best classifier so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf2, y_bina, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC3 TFIDF2 with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_rfc3 = ClassifierChain(RandomForestClassifier())\n",
    "chain_rfc3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.02      0.04       376\n",
      "          1       0.00      0.00      0.00       243\n",
      "          2       0.07      0.00      0.01       222\n",
      "          3       0.25      0.01      0.02       124\n",
      "          4       0.28      0.03      0.05       286\n",
      "          5       0.62      0.05      0.09       102\n",
      "          6       0.40      0.03      0.05       290\n",
      "          7       0.35      0.04      0.08       163\n",
      "          8       0.00      0.00      0.00       177\n",
      "          9       0.30      0.03      0.06       433\n",
      "         10       0.49      0.05      0.10       422\n",
      "         11       0.12      0.01      0.01       140\n",
      "         12       0.29      0.02      0.04       357\n",
      "         13       0.33      0.02      0.04        49\n",
      "         14       0.00      0.00      0.00       266\n",
      "         15       0.14      0.01      0.01       276\n",
      "         16       0.45      0.04      0.07       561\n",
      "         17       0.11      0.01      0.01       177\n",
      "         18       0.32      0.03      0.05       289\n",
      "         19       0.20      0.01      0.03       274\n",
      "         20       0.25      0.01      0.02       335\n",
      "         21       0.00      0.00      0.00       249\n",
      "         22       0.44      0.05      0.09       294\n",
      "         23       0.00      0.00      0.00       109\n",
      "         24       0.25      0.01      0.02       280\n",
      "         25       0.17      0.01      0.02       102\n",
      "         26       0.29      0.04      0.07       324\n",
      "         27       0.71      0.06      0.11       172\n",
      "         28       0.00      0.00      0.00        88\n",
      "         29       0.12      0.03      0.05       153\n",
      "         30       0.41      0.04      0.08       213\n",
      "         31       0.00      0.00      0.00       295\n",
      "\n",
      "avg / total       0.26      0.02      0.04      7841\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_rfc3 = chain_rfc3.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rfc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(chain_rfc3, open('moods_chain_rfc3.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC4 TFIDF2 without Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=5 ..........\n",
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=5, score=0.016302 -   7.0s\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=5 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=5, score=0.017739 -   6.5s\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=5 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   13.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=5, score=0.018455 -   7.1s\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=10, score=0.008957 -  11.3s\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=10, score=0.008421 -  11.1s\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=10, score=0.008959 -  10.8s\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=100, score=0.005554 - 1.7min\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=100, score=0.005196 - 1.7min\n",
      "[CV] max_features=sqrt, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=2, n_estimators=100, score=0.006271 - 1.7min\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=5, score=0.016123 -   3.2s\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=5, score=0.015230 -   3.2s\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=5, score=0.018276 -   3.2s\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=10, score=0.009674 -   6.2s\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=10, score=0.009855 -   6.2s\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=10, score=0.012184 -   6.3s\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=100, score=0.004479 - 1.0min\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=100, score=0.004838 - 1.1min\n",
      "[CV] max_features=sqrt, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=5, n_estimators=100, score=0.005913 - 1.1min\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=5, score=0.014869 -   3.0s\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=5, score=0.014155 -   3.0s\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=5, score=0.013797 -   2.9s\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=10, score=0.010391 -   5.9s\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=10, score=0.006092 -   6.9s\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=10, score=0.008242 -   6.6s\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=100, score=0.004479 - 1.1min\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=100, score=0.002688 - 1.1min\n",
      "[CV] max_features=sqrt, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=sqrt, min_samples_split=10, n_estimators=100, score=0.004300 - 1.1min\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=5 ..........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=5, score=0.011107 -   5.1s\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=5 ..........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=5, score=0.011467 -   4.3s\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=5 ..........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=5, score=0.013259 -   3.8s\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=10, score=0.005374 -   8.3s\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=10, score=0.005196 -   8.9s\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=10, score=0.005375 -  11.2s\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=100, score=0.004120 - 1.6min\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=100, score=0.003225 - 1.5min\n",
      "[CV] max_features=log2, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=log2, min_samples_split=2, n_estimators=100, score=0.004121 - 1.4min\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=5, score=0.010749 -   2.2s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=5, score=0.008242 -   1.7s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=5, score=0.005555 -   1.8s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=10, score=0.004837 -   3.6s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=10, score=0.004479 -   3.7s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=10, score=0.006450 -   3.8s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=100, score=0.003941 -  32.9s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=100, score=0.002688 -  30.8s\n",
      "[CV] max_features=log2, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=log2, min_samples_split=5, n_estimators=100, score=0.004121 -  32.0s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=5, score=0.007524 -   1.7s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=5, score=0.006092 -   1.5s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=5, score=0.005375 -   1.6s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=10, score=0.005554 -   3.0s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=10, score=0.004300 -   3.0s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=10, score=0.004300 -   3.6s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=100, score=0.003583 -  32.7s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=100, score=0.001792 -  29.1s\n",
      "[CV] max_features=log2, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=log2, min_samples_split=10, n_estimators=100, score=0.002329 -  29.9s\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=5 ..........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=5, score=0.018631 -   5.9s\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=5 ..........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=5, score=0.017022 -   5.8s\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=5 ..........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=5, score=0.025085 -   5.2s\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=10, score=0.009674 -  10.5s\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=10, score=0.010034 -  10.9s\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=10 .........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=10, score=0.008601 -  13.3s\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=100, score=0.005016 - 1.8min\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=100, score=0.004659 - 2.0min\n",
      "[CV] max_features=auto, min_samples_split=2, n_estimators=100 ........\n",
      "[CV]  max_features=auto, min_samples_split=2, n_estimators=100, score=0.005734 - 1.8min\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=5, score=0.015048 -   3.8s\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=5, score=0.013080 -   4.2s\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=5 ..........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=5, score=0.018814 -   4.1s\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=10, score=0.010749 -   7.1s\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=10, score=0.008959 -   7.1s\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=10 .........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=10, score=0.011109 -   7.1s\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=100, score=0.005733 - 1.1min\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=100, score=0.004479 - 1.1min\n",
      "[CV] max_features=auto, min_samples_split=5, n_estimators=100 ........\n",
      "[CV]  max_features=auto, min_samples_split=5, n_estimators=100, score=0.006630 - 1.1min\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=5, score=0.012899 -   4.1s\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=5, score=0.014334 -   3.3s\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=5 .........\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=5, score=0.015051 -   3.1s\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=10, score=0.007166 -   5.7s\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=10, score=0.006630 -   5.8s\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=10 ........\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=10, score=0.009138 -   5.9s\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=100, score=0.003762 - 1.0min\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=100, score=0.003404 - 1.2min\n",
      "[CV] max_features=auto, min_samples_split=10, n_estimators=100 .......\n",
      "[CV]  max_features=auto, min_samples_split=10, n_estimators=100, score=0.003942 - 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed: 36.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 100], 'min_samples_split': [2, 5, 10], 'max_features': ['sqrt', 'log2', 'auto']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=3)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc4 = RandomForestClassifier()\n",
    "parameters_rfc4 = {'n_estimators':[5, 10, 100], 'min_samples_split':[2, 5, 10], 'max_features':['sqrt', 'log2', 'auto']}\n",
    "grid_rfc4 = GridSearchCV(rfc4, parameters_rfc4, verbose=3)\n",
    "grid_rfc4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.09      0.15       376\n",
      "          1       0.16      0.01      0.02       243\n",
      "          2       0.15      0.01      0.02       222\n",
      "          3       0.00      0.00      0.00       124\n",
      "          4       0.17      0.02      0.04       286\n",
      "          5       0.58      0.07      0.12       102\n",
      "          6       0.24      0.02      0.04       290\n",
      "          7       0.36      0.03      0.06       163\n",
      "          8       0.12      0.01      0.01       177\n",
      "          9       0.19      0.03      0.06       433\n",
      "         10       0.38      0.13      0.20       422\n",
      "         11       0.00      0.00      0.00       140\n",
      "         12       0.34      0.03      0.06       357\n",
      "         13       0.25      0.02      0.04        49\n",
      "         14       0.11      0.01      0.01       266\n",
      "         15       0.19      0.01      0.02       276\n",
      "         16       0.26      0.04      0.08       561\n",
      "         17       0.05      0.01      0.01       177\n",
      "         18       0.21      0.02      0.04       289\n",
      "         19       0.13      0.01      0.01       274\n",
      "         20       0.10      0.01      0.02       335\n",
      "         21       0.00      0.00      0.00       249\n",
      "         22       0.36      0.07      0.11       294\n",
      "         23       0.33      0.02      0.03       109\n",
      "         24       0.24      0.02      0.04       280\n",
      "         25       0.33      0.02      0.04       102\n",
      "         26       0.24      0.06      0.09       324\n",
      "         27       0.54      0.04      0.08       172\n",
      "         28       1.00      0.01      0.02        88\n",
      "         29       0.18      0.02      0.04       153\n",
      "         30       0.29      0.03      0.06       213\n",
      "         31       0.08      0.01      0.01       295\n",
      "\n",
      "avg / total       0.24      0.03      0.06      7841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grid_rfc4.best_estimator_)\n",
    "predictions_rfc4 = grid_rfc4.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rfc4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is similar to the one with classifier chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about bag of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12138)\t1\n",
      "  (0, 1088)\t2\n",
      "  (0, 6176)\t21\n",
      "  (0, 20938)\t6\n",
      "  (0, 20459)\t1\n",
      "  (0, 11891)\t1\n",
      "  (0, 11285)\t2\n",
      "  (0, 1081)\t4\n",
      "  (0, 8170)\t2\n",
      "  (0, 17042)\t6\n",
      "  (0, 12341)\t6\n",
      "  (0, 20937)\t6\n",
      "  (0, 6175)\t30\n",
      "  (0, 17515)\t40\n",
      "  (0, 7993)\t2\n",
      "  (0, 10216)\t8\n",
      "  (0, 16653)\t8\n",
      "  (0, 22624)\t17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), min_df=10)\n",
    "bow = count_vect.fit_transform(new_lyrics['clean_lyrics'])\n",
    "print(bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(count_vect, open('bow_count_vect.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y_bina, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC5 BOW with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_rfc5 = ClassifierChain(RandomForestClassifier())\n",
    "chain_rfc5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.01      0.02       195\n",
      "          1       0.00      0.00      0.00       118\n",
      "          2       0.00      0.00      0.00       118\n",
      "          3       0.00      0.00      0.00        63\n",
      "          4       0.25      0.02      0.04       151\n",
      "          5       0.33      0.04      0.08        45\n",
      "          6       0.42      0.03      0.06       162\n",
      "          7       0.33      0.02      0.04        88\n",
      "          8       0.20      0.01      0.02        91\n",
      "          9       0.41      0.04      0.08       201\n",
      "         10       0.39      0.05      0.09       187\n",
      "         11       0.00      0.00      0.00        75\n",
      "         12       0.08      0.01      0.01       196\n",
      "         13       0.00      0.00      0.00        20\n",
      "         14       0.12      0.01      0.02       122\n",
      "         15       0.12      0.01      0.01       144\n",
      "         16       0.23      0.01      0.02       267\n",
      "         17       0.00      0.00      0.00        95\n",
      "         18       0.27      0.02      0.04       140\n",
      "         19       0.22      0.01      0.03       145\n",
      "         20       0.42      0.03      0.05       177\n",
      "         21       0.00      0.00      0.00       121\n",
      "         22       0.27      0.02      0.04       135\n",
      "         23       0.20      0.02      0.03        54\n",
      "         24       0.50      0.02      0.04       145\n",
      "         25       0.50      0.04      0.07        53\n",
      "         26       0.33      0.04      0.07       149\n",
      "         27       0.67      0.05      0.09        81\n",
      "         28       0.00      0.00      0.00        45\n",
      "         29       0.11      0.03      0.04        75\n",
      "         30       0.36      0.04      0.07        98\n",
      "         31       0.11      0.01      0.01       154\n",
      "\n",
      "avg / total       0.24      0.02      0.04      3910\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_rfc5 = chain_rfc5.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rfc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(chain_rfc5, open('moods_chain_rfc5.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is similar to what we got with TF-IDF and RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN3 BOW with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_knn3 = ClassifierChain(KNeighborsClassifier())\n",
    "chain_knn3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.03      0.06       195\n",
      "          1       0.00      0.00      0.00       118\n",
      "          2       0.10      0.05      0.07       118\n",
      "          3       0.00      0.00      0.00        63\n",
      "          4       0.25      0.01      0.01       151\n",
      "          5       0.00      0.00      0.00        45\n",
      "          6       0.67      0.04      0.07       162\n",
      "          7       0.00      0.00      0.00        88\n",
      "          8       0.00      0.00      0.00        91\n",
      "          9       0.24      0.02      0.05       201\n",
      "         10       0.38      0.03      0.06       187\n",
      "         11       0.00      0.00      0.00        75\n",
      "         12       0.20      0.01      0.02       196\n",
      "         13       0.00      0.00      0.00        20\n",
      "         14       0.00      0.00      0.00       122\n",
      "         15       0.00      0.00      0.00       144\n",
      "         16       0.23      0.06      0.10       267\n",
      "         17       1.00      0.01      0.02        95\n",
      "         18       0.21      0.02      0.04       140\n",
      "         19       0.29      0.03      0.05       145\n",
      "         20       0.06      0.01      0.01       177\n",
      "         21       0.11      0.01      0.02       121\n",
      "         22       0.19      0.04      0.06       135\n",
      "         23       0.00      0.00      0.00        54\n",
      "         24       0.08      0.01      0.01       145\n",
      "         25       0.00      0.00      0.00        53\n",
      "         26       0.15      0.05      0.07       149\n",
      "         27       0.17      0.04      0.06        81\n",
      "         28       0.00      0.00      0.00        45\n",
      "         29       0.05      0.09      0.07        75\n",
      "         30       0.08      0.01      0.02        98\n",
      "         31       0.00      0.00      0.00       154\n",
      "\n",
      "avg / total       0.18      0.02      0.03      3910\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_knn3 = chain_knn3.predict(X_test)\n",
    "print(classification_report(y_test, predictions_knn3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to first scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to filter out the non-English data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove foreign languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19434, 3)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lyrics = lyrics.copy()\n",
    "\n",
    "for index, row in en_lyrics.iterrows():\n",
    "    if detect(row['lyrics_features']) != 'en':\n",
    "        en_lyrics.drop(axis=0, index=index, inplace=True)\n",
    "        \n",
    "en_lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lyrics</th>\n",
       "      <th>moods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>late ve ve lose sleep dream thing babi ve ve p...</td>\n",
       "      <td>[happy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parti rock yeah woo let s parti rock hous toni...</td>\n",
       "      <td>[happy, celebratory, rowdy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j lo s new generat mr worldwid parti peopl flo...</td>\n",
       "      <td>[energetic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>today don t feel like do just wanna lay bed do...</td>\n",
       "      <td>[happy, sprightly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s start heart reach fever pitch s bring dark f...</td>\n",
       "      <td>[warm]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_lyrics  \\\n",
       "1  late ve ve lose sleep dream thing babi ve ve p...   \n",
       "2  parti rock yeah woo let s parti rock hous toni...   \n",
       "4  j lo s new generat mr worldwid parti peopl flo...   \n",
       "5  today don t feel like do just wanna lay bed do...   \n",
       "6  s start heart reach fever pitch s bring dark f...   \n",
       "\n",
       "                         moods  \n",
       "1                      [happy]  \n",
       "2  [happy, celebratory, rowdy]  \n",
       "4                  [energetic]  \n",
       "5           [happy, sprightly]  \n",
       "6                       [warm]  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols2 = ['clean_lyrics', 'moods']\n",
    "en_lyrics = en_lyrics[cols2]\n",
    "en_lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay seems like it worked :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = en_lyrics['moods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb_en = MultiLabelBinarizer()\n",
    "y_bina_en = mlb_en.fit_transform(y)\n",
    "y_bina_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this dataset on the RFC + chain classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF-EN with EN lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11362)\t0.10513618837496067\n",
      "  (0, 22935)\t0.23442870962670864\n",
      "  (0, 13315)\t0.10016971771099573\n",
      "  (0, 19491)\t0.0958732908391669\n",
      "  (0, 5327)\t0.08106395649830213\n",
      "  (0, 21302)\t0.09452304237166369\n",
      "  (0, 938)\t0.06591460900993013\n",
      "  (0, 16924)\t0.11908067959139214\n",
      "  (0, 8821)\t0.08359707172173109\n",
      "  (0, 18374)\t0.07301417160224398\n",
      "  (0, 3883)\t0.26870204851664475\n",
      "  (0, 5001)\t0.12352866499092034\n",
      "  (0, 12761)\t0.08115109606283256\n",
      "  (0, 20127)\t0.11269191693996185\n",
      "  (0, 24806)\t0.00937740337197313\n",
      "  (0, 11887)\t0.009822064274593907\n",
      "  (0, 12079)\t0.006784947007571454\n",
      "  (0, 20849)\t0.038236528139323975\n",
      "  (0, 23118)\t0.025349425937520324\n",
      "  (0, 9106)\t0.009889898882385392\n",
      "  (0, 12529)\t0.013675339203137295\n",
      "  (0, 6037)\t0.011518034461276734\n",
      "  (0, 6643)\t0.020639093942934686\n",
      "  (0, 19290)\t0.017036516044442562\n",
      "  (0, 18869)\t0.02179677278162771\n",
      "  :\t:\n",
      "  (0, 11946)\t0.023432806319843767\n",
      "  (0, 12536)\t0.03092843131264083\n",
      "  (0, 15987)\t0.058150235519397284\n",
      "  (0, 5185)\t0.03739762350008375\n",
      "  (0, 21528)\t0.06095764374890914\n",
      "  (0, 10427)\t0.05978077476829703\n",
      "  (0, 6409)\t0.02154360146488341\n",
      "  (0, 4936)\t0.029380662243481483\n",
      "  (0, 24688)\t0.029075117759698642\n",
      "  (0, 21330)\t0.027626504089135546\n",
      "  (0, 6455)\t0.05978077476829703\n",
      "  (0, 4932)\t0.05876132448696297\n",
      "  (0, 17951)\t0.05257024821032558\n",
      "  (0, 10755)\t0.06287631290661573\n",
      "  (0, 13912)\t0.03489274818292931\n",
      "  (0, 6302)\t0.052858378119132424\n",
      "  (0, 12891)\t0.06481601281924441\n",
      "  (0, 6383)\t0.022990645212243532\n",
      "  (0, 13503)\t0.02192994680649159\n",
      "  (0, 6317)\t0.029712808007140105\n",
      "  (0, 24437)\t0.029380662243481483\n",
      "  (0, 13969)\t0.024377817173769502\n",
      "  (0, 23699)\t0.2706896927429946\n",
      "  (0, 14046)\t0.021952601283751223\n",
      "  (0, 23368)\t0.03027243278926798\n"
     ]
    }
   ],
   "source": [
    "vectorizer_en = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "tf_idf_en = vectorizer_en.fit_transform(en_lyrics['clean_lyrics'])\n",
    "print(tf_idf_en[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer_en, open('tfidf_vectorizer_en.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_en = tf_idf_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_en, y_bina_en, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC_EN1 TFIDF with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_rfc_en1 = ClassifierChain(RandomForestClassifier())\n",
    "chain_rfc_en1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.03      0.05       360\n",
      "          1       0.00      0.00      0.00       232\n",
      "          2       0.21      0.01      0.03       214\n",
      "          3       0.00      0.00      0.00       132\n",
      "          4       0.21      0.02      0.04       255\n",
      "          5       0.40      0.05      0.08        85\n",
      "          6       0.29      0.02      0.04       271\n",
      "          7       0.20      0.01      0.02       174\n",
      "          8       0.12      0.01      0.01       152\n",
      "          9       0.33      0.03      0.06       391\n",
      "         10       0.39      0.05      0.09       393\n",
      "         11       0.25      0.01      0.01       177\n",
      "         12       0.22      0.02      0.04       315\n",
      "         13       0.50      0.02      0.04        48\n",
      "         14       0.31      0.02      0.03       284\n",
      "         15       0.18      0.01      0.02       239\n",
      "         16       0.26      0.02      0.04       522\n",
      "         17       0.25      0.01      0.02       186\n",
      "         18       0.00      0.00      0.00       254\n",
      "         19       0.24      0.02      0.03       265\n",
      "         20       0.12      0.01      0.01       313\n",
      "         21       0.29      0.01      0.02       229\n",
      "         22       0.25      0.01      0.03       220\n",
      "         23       0.00      0.00      0.00        98\n",
      "         24       0.20      0.01      0.02       276\n",
      "         25       0.00      0.00      0.00        91\n",
      "         26       0.46      0.02      0.04       279\n",
      "         27       0.12      0.01      0.01       130\n",
      "         28       0.00      0.00      0.00       102\n",
      "         29       0.20      0.01      0.01       135\n",
      "         30       0.18      0.01      0.02       205\n",
      "         31       0.06      0.00      0.01       273\n",
      "\n",
      "avg / total       0.23      0.02      0.03      7300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rfc_en1 = chain_rfc_en1.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rfc_en1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(chain_rfc_en1, open('moods_chain_rfc_en1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar score to before :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN-EN1 TFIDF with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_knn_en1 = ClassifierChain(KNeighborsClassifier())\n",
    "chain_knn_en1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.03      0.05       360\n",
      "          1       0.00      0.00      0.00       232\n",
      "          2       0.00      0.00      0.00       214\n",
      "          3       0.00      0.00      0.00       132\n",
      "          4       0.57      0.02      0.03       255\n",
      "          5       1.00      0.04      0.07        85\n",
      "          6       0.38      0.02      0.04       271\n",
      "          7       0.00      0.00      0.00       174\n",
      "          8       0.00      0.00      0.00       152\n",
      "          9       0.29      0.02      0.03       391\n",
      "         10       0.22      0.32      0.26       393\n",
      "         11       0.00      0.00      0.00       177\n",
      "         12       0.16      0.03      0.06       315\n",
      "         13       0.00      0.00      0.00        48\n",
      "         14       0.24      0.03      0.06       284\n",
      "         15       0.18      0.02      0.04       239\n",
      "         16       0.24      0.08      0.12       522\n",
      "         17       0.18      0.02      0.03       186\n",
      "         18       0.08      0.02      0.03       254\n",
      "         19       0.09      0.02      0.03       265\n",
      "         20       0.11      0.06      0.08       313\n",
      "         21       0.14      0.01      0.02       229\n",
      "         22       0.11      0.06      0.08       220\n",
      "         23       0.23      0.14      0.18        98\n",
      "         24       0.15      0.06      0.09       276\n",
      "         25       0.00      0.00      0.00        91\n",
      "         26       0.11      0.35      0.17       279\n",
      "         27       0.09      0.12      0.10       130\n",
      "         28       0.13      0.03      0.05       102\n",
      "         29       0.05      0.19      0.08       135\n",
      "         30       0.19      0.05      0.08       205\n",
      "         31       0.06      0.23      0.10       273\n",
      "\n",
      "avg / total       0.17      0.07      0.07      7300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_knn_en1 = chain_knn_en1.predict(X_test)\n",
    "print(classification_report(y_test, predictions_knn_en1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even lower :((((("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW-EN with EN lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 23368)\t1\n",
      "  (0, 14046)\t1\n",
      "  (0, 23699)\t9\n",
      "  (0, 13969)\t1\n",
      "  (0, 24437)\t1\n",
      "  (0, 6317)\t1\n",
      "  (0, 13503)\t1\n",
      "  (0, 6383)\t1\n",
      "  (0, 12891)\t3\n",
      "  (0, 6302)\t2\n",
      "  (0, 13912)\t2\n",
      "  (0, 10755)\t2\n",
      "  (0, 17951)\t2\n",
      "  (0, 4932)\t2\n",
      "  (0, 6455)\t2\n",
      "  (0, 21330)\t1\n",
      "  (0, 24688)\t1\n",
      "  (0, 4936)\t1\n",
      "  (0, 6409)\t1\n",
      "  (0, 10427)\t2\n",
      "  (0, 21528)\t2\n",
      "  (0, 5185)\t2\n",
      "  (0, 15987)\t2\n",
      "  (0, 12536)\t1\n",
      "  (0, 11946)\t1\n",
      "  :\t:\n",
      "  (0, 18869)\t1\n",
      "  (0, 19290)\t1\n",
      "  (0, 6643)\t1\n",
      "  (0, 6037)\t1\n",
      "  (0, 12529)\t1\n",
      "  (0, 9106)\t1\n",
      "  (0, 23118)\t1\n",
      "  (0, 20849)\t2\n",
      "  (0, 12079)\t1\n",
      "  (0, 11887)\t1\n",
      "  (0, 24806)\t1\n",
      "  (0, 20127)\t8\n",
      "  (0, 12761)\t11\n",
      "  (0, 5001)\t7\n",
      "  (0, 3883)\t15\n",
      "  (0, 18374)\t7\n",
      "  (0, 8821)\t7\n",
      "  (0, 16924)\t7\n",
      "  (0, 938)\t7\n",
      "  (0, 21302)\t10\n",
      "  (0, 5327)\t7\n",
      "  (0, 19491)\t7\n",
      "  (0, 13315)\t7\n",
      "  (0, 22935)\t28\n",
      "  (0, 11362)\t7\n"
     ]
    }
   ],
   "source": [
    "count_vect_en = CountVectorizer(ngram_range=(1, 2), min_df=10)\n",
    "bow_en = count_vect_en.fit_transform(en_lyrics['clean_lyrics'])\n",
    "print(bow_en[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow_en = bow_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_en, y_bina_en, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC_EN2 BOW with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_rfc_en2 = ClassifierChain(RandomForestClassifier())\n",
    "chain_rfc_en2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.02      0.03       316\n",
      "          1       0.20      0.00      0.01       253\n",
      "          2       0.07      0.00      0.01       205\n",
      "          3       0.11      0.01      0.01       141\n",
      "          4       0.07      0.01      0.01       248\n",
      "          5       0.38      0.06      0.11        82\n",
      "          6       0.40      0.02      0.04       270\n",
      "          7       0.23      0.02      0.04       153\n",
      "          8       0.33      0.01      0.02       173\n",
      "          9       0.48      0.06      0.10       416\n",
      "         10       0.23      0.02      0.04       414\n",
      "         11       0.07      0.01      0.01       152\n",
      "         12       0.21      0.02      0.03       297\n",
      "         13       0.00      0.00      0.00        56\n",
      "         14       0.14      0.01      0.01       260\n",
      "         15       0.15      0.01      0.01       257\n",
      "         16       0.35      0.02      0.05       534\n",
      "         17       0.08      0.01      0.01       179\n",
      "         18       0.16      0.01      0.02       270\n",
      "         19       0.09      0.00      0.01       248\n",
      "         20       0.14      0.01      0.02       334\n",
      "         21       0.25      0.01      0.02       244\n",
      "         22       0.45      0.02      0.04       230\n",
      "         23       0.33      0.02      0.04        93\n",
      "         24       0.10      0.00      0.01       275\n",
      "         25       0.20      0.01      0.02        94\n",
      "         26       0.29      0.02      0.03       285\n",
      "         27       0.20      0.01      0.01       148\n",
      "         28       0.00      0.00      0.00        91\n",
      "         29       0.22      0.01      0.02       154\n",
      "         30       0.20      0.01      0.02       201\n",
      "         31       0.10      0.00      0.01       272\n",
      "\n",
      "avg / total       0.23      0.02      0.03      7345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rfc_en2 = chain_rfc_en2.predict(X_test)\n",
    "print(classification_report(y_test, predictions_rfc_en2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN_EN2 BOW with Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform'),\n",
       "        cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_knn_en2 = ClassifierChain(KNeighborsClassifier(n_neighbors=10))\n",
    "chain_knn_en2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.01      0.02       360\n",
      "          1       0.00      0.00      0.00       232\n",
      "          2       0.00      0.00      0.00       214\n",
      "          3       0.00      0.00      0.00       132\n",
      "          4       1.00      0.00      0.01       255\n",
      "          5       0.00      0.00      0.00        85\n",
      "          6       1.00      0.01      0.01       271\n",
      "          7       0.00      0.00      0.00       174\n",
      "          8       0.00      0.00      0.00       152\n",
      "          9       0.00      0.00      0.00       391\n",
      "         10       0.50      0.01      0.02       393\n",
      "         11       0.00      0.00      0.00       177\n",
      "         12       0.00      0.00      0.00       315\n",
      "         13       0.00      0.00      0.00        48\n",
      "         14       0.00      0.00      0.00       284\n",
      "         15       0.00      0.00      0.00       239\n",
      "         16       0.06      0.00      0.00       522\n",
      "         17       0.00      0.00      0.00       186\n",
      "         18       0.00      0.00      0.00       254\n",
      "         19       0.00      0.00      0.00       265\n",
      "         20       1.00      0.01      0.01       313\n",
      "         21       0.00      0.00      0.00       229\n",
      "         22       0.00      0.00      0.00       220\n",
      "         23       0.00      0.00      0.00        98\n",
      "         24       0.00      0.00      0.00       276\n",
      "         25       0.00      0.00      0.00        91\n",
      "         26       0.00      0.00      0.00       279\n",
      "         27       0.00      0.00      0.00       130\n",
      "         28       0.00      0.00      0.00       102\n",
      "         29       0.00      0.00      0.00       135\n",
      "         30       0.00      0.00      0.00       205\n",
      "         31       0.00      0.00      0.00       273\n",
      "\n",
      "avg / total       0.20      0.00      0.00      7300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions_knn_en2 = chain_knn_en2.predict(X_test)\n",
    "print(classification_report(y_test, predictions_knn_en2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(lyrics):\n",
    "#     Clean text\n",
    "    clean_lyrics = clean_text(lyrics)\n",
    "    lyrics_list = []\n",
    "    lyrics_list.append(clean_lyrics)\n",
    "    \n",
    "#     First classifier: tf-idf chain_rf3\n",
    "    format_rfc3 = vectorizer2.transform(lyrics_list)\n",
    "    moods_rfc3 = chain_rfc3.predict_proba(format_rfc3)\n",
    "    \n",
    "#    Second classifier: bow chain_rfc5\n",
    "    format_rfc5 = count_vect.transform(lyrics_list)\n",
    "    moods_rfc5 = chain_rfc5.predict_proba(format_rfc5)\n",
    "    \n",
    "#     Third classifier: \n",
    "    format_rfc_en1 = vectorizer_en.transform(lyrics_list)\n",
    "    moods_rfc_en1 = chain_rfc_en1.predict_proba(format_rfc_en1)\n",
    "    \n",
    "#     Put all in a mood list\n",
    "    all_moods_list = pd.DataFrame(\n",
    "    {'mood': mlb.classes_.tolist(),\n",
    "     'rfc3': moods_rfc3.tolist()[0],\n",
    "     'rfc5': moods_rfc5.tolist()[0],\n",
    "     'rfc_en1': moods_rfc_en1.tolist()[0]\n",
    "    })\n",
    "    \n",
    "#     Calculate the max score\n",
    "    all_moods_list['max'] = all_moods_list.max(axis=1)\n",
    "    all_moods_list.sort_values('max', axis=0, ascending=False, inplace=True)\n",
    "    all_moods_list.reset_index(drop=True, inplace=True)\n",
    "#     print(all_moods_list)\n",
    "    \n",
    "#     Select relevant moods\n",
    "    if all_moods_list.loc[0,('max')] >= 0.3:\n",
    "        mood_1 = all_moods_list['mood'][0]\n",
    "        if all_moods_list.loc[1,('max')] >= 0.3:\n",
    "            mood_2 = all_moods_list['mood'][1]\n",
    "            if all_moods_list.loc[1,('max')] >= 0.3:\n",
    "                mood_3 = all_moods_list['mood'][2]\n",
    "            else:\n",
    "                mood_3 = \"\"\n",
    "        else:\n",
    "            mood_2 = \"\"\n",
    "            mood_3 = \"\"\n",
    "    else:\n",
    "        mood_1 = all_moods_list['mood'][0]\n",
    "        mood_2 = \"\"\n",
    "        mood_3 = \"\"\n",
    "        \n",
    "#     print(mood_1)\n",
    "#     print(mood_2)\n",
    "#     print(mood_3)\n",
    "\n",
    "    final_moods = []\n",
    "    final_moods.append(mood_1)\n",
    "    if mood_2 != \"\":\n",
    "        final_moods.append(mood_2)\n",
    "    if mood_3 != \"\":\n",
    "        final_moods.append(mood_3)\n",
    "            \n",
    "    return final_moods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da da da da da da da da da da da da da da you re sweet like chocolate boy sweet like chocolate you bring me so much joy you re sweet like chocolate boy finding a way in the dark ain t so hard when you re close to my heart you are there when i m feeling alone all i need is for you to come home chorus you re sweet like chocolate boy sweet like chocolate you bring me so much joy you re sweet like chocolate boy trust is the lock is the key there s no doubt that your love s all for me you are sweet on the tip of my tongue you are warm like the rays of the sun chorus you re sweet like chocolate boy sweet like chocolate you bring me so much joy you re sweet like chocolate boy you re sweet like da da da da da da knowing you re there every day makes me high in my own special way i am caught in the face of your love holding you is a gift from above you re sweet like chocolate boy sweet like chocolate you bring me so much joy you re sweet like chocolate boy you re sweet like da da da sweet like chocolate sweet like chocolate boy repeat to fade\n",
      "['gloomy', 'cold', 'visceral', 'spacey']\n"
     ]
    }
   ],
   "source": [
    "n = 12345\n",
    "print(lyrics['lyrics_features'][n])\n",
    "print(lyrics['moods'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlolo = \"\"\"Oh, oh, oh, little China girl\n",
    "Oh, oh, oh, little China girl\n",
    "I could escape this feeling, with my China girl\n",
    "I feel a wreck without my, little China girl\n",
    "I hear her heart beating, loud as thunder\n",
    "Saw they stars crashing\n",
    "I'm a mess without my, little China girl\n",
    "Wake up mornings where's my, little China girl\n",
    "I hear her heart's beating, loud as thunder\n",
    "Saw they stars crashing down\n",
    "I feel a-tragic like I'm Marlon Brando\n",
    "When I look at my China girl\n",
    "I could pretend that nothing really meant too much\n",
    "When I look at my China girl\n",
    "I stumble into town just like a sacred cow\n",
    "Visions of swastikas in my head\n",
    "Plans for everyone\n",
    "It's in the whites of my eyes\n",
    "My little China girl\n",
    "You shouldn't mess with me\n",
    "I'll ruin everything you are\n",
    "You know, I'll give you television\n",
    "I'll give you eyes of blue\n",
    "I'll give you men's who want to rule the world\n",
    "And when I get excited\n",
    "My little China girl says\n",
    "Oh baby, just you shut your mouth\n",
    "She says, sh-sh-shhh\n",
    "She says, sh-sh-shhh\n",
    "She says\n",
    "She says\n",
    "And when I get excited\n",
    "My little China girl says\n",
    "Oh baby, just you shut your mouth\n",
    "And when I get excited\n",
    "My little China girl says\n",
    "Oh baby, just you shut your mouth\n",
    "She says, sh-sh-shhh\n",
    "She says\n",
    "Oh, oh, oh, little China girl\n",
    "Oh, oh, oh, little China girl\n",
    "Oh, oh, oh, little China girl\n",
    "Oh, oh, oh, little China girl\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sprightly', 'celebratory', 'cocky']"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moods_proba = model(testlolo)\n",
    "mood_classes = mlb.classes_.tolist()\n",
    "moods_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moods_model(lyrics):\n",
    "#     Clean text\n",
    "    clean_lyrics = clean_text(lyrics)\n",
    "    lyrics_list = []\n",
    "    lyrics_list.append(clean_lyrics)\n",
    "    \n",
    "#     First classifier: tf-idf chain_rf3\n",
    "    lyrics1 = moods_vect1.transform(lyrics_list)\n",
    "    moods_proba1 = moods_cl1.predict_proba(lyrics1)\n",
    "    \n",
    "#    Second classifier: bow chain_rfc5\n",
    "    lyrics2 = moods_vect2.transform(lyrics_list)\n",
    "    moods_proba2 = moods_cl2.predict_proba(lyrics2)\n",
    "    \n",
    "#     Third classifier: \n",
    "    lyrics3 = moods_vect3.transform(lyrics_list)\n",
    "    moods_proba3 = moods_cl3.predict_proba(lyrics3)\n",
    "    \n",
    "#     Put all in a mood list\n",
    "    all_moods_list = pd.DataFrame(\n",
    "    {'mood': mlb.classes_.tolist(),\n",
    "     'cl1': moods_proba1.tolist()[0],\n",
    "     'cl2': moods_proba2.tolist()[0],\n",
    "     'cl3': moods_proba3.tolist()[0]\n",
    "    })\n",
    "    \n",
    "#     Calculate the max score\n",
    "    all_moods_list['max'] = all_moods_list.max(axis=1)\n",
    "    all_moods_list.sort_values('max', axis=0, ascending=False, inplace=True)\n",
    "    all_moods_list.reset_index(drop=True, inplace=True)\n",
    "#     print(all_moods_list)\n",
    "    \n",
    "#     Select relevant moods\n",
    "    if all_moods_list.loc[0,('max')] >= 0.3:\n",
    "        mood_1 = all_moods_list['mood'][0]\n",
    "        if all_moods_list.loc[1,('max')] >= 0.3:\n",
    "            mood_2 = all_moods_list['mood'][1]\n",
    "            if all_moods_list.loc[1,('max')] >= 0.3:\n",
    "                mood_3 = all_moods_list['mood'][2]\n",
    "            else:\n",
    "                mood_3 = \"\"\n",
    "        else:\n",
    "            mood_2 = \"\"\n",
    "            mood_3 = \"\"\n",
    "    else:\n",
    "        mood_1 = all_moods_list['mood'][0]\n",
    "        mood_2 = \"\"\n",
    "        mood_3 = \"\"\n",
    "        \n",
    "#     print(mood_1)\n",
    "#     print(mood_2)\n",
    "#     print(mood_3)\n",
    "\n",
    "    final_moods = []\n",
    "    final_moods.append(mood_1)\n",
    "    if mood_2 != \"\":\n",
    "        final_moods.append(mood_2)\n",
    "    if mood_3 != \"\":\n",
    "        final_moods.append(mood_3)\n",
    "            \n",
    "    return final_moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(moods_model, open('moods_model_function.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
