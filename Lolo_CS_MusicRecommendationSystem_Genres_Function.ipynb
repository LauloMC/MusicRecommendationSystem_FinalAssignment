{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoderSchool Final Project Genres\n",
    "## Music Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_json('MasterSongList.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove the list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df2 = full_df.copy()\n",
    "full_df2['genres'] = full_df2['genres'].apply(''.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And only want to keep the first genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_first_genre(genre):\n",
    "    if len(genre) > 0:\n",
    "        return genre.split(':')[0]\n",
    "    else:\n",
    "        return genre\n",
    "\n",
    "full_df2['genres'] = full_df2['genres'].apply(split_first_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now only want to keep the audio features and the genre, let's create a new dataframe: df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_headers = ['key', 'energy', 'liveliness', 'tempo', 'speechiness', 'acousticness', 'instrumentalness', 'time_signature', 'duration', 'loudness', 'valence', 'danceability', 'mode', 'time_signature_confidence', 'tempo_confidence', 'key_confidence', 'mode_confidence']\n",
    "features_list = full_df2['audio_features'].tolist()\n",
    "df = pd.DataFrame(features_list, columns=features_headers)\n",
    "df['genres'] = full_df2['genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the songs with no genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['genres'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the NaN rows and their distribution among the genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checknan(x):\n",
    "    return np.isnan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = ['bluegrass', 'blues & blues rock', \"children's\", 'christian', 'classical', 'country', 'dance', \"dubstep & drum 'n' bass\", 'easy listening', 'electronica', 'film scores', 'folk', 'funk', 'hawaiian ', 'indie', \"int'l\", 'international/world', 'jazz', 'latin', 'nature sounds', 'oldies', 'pop', 'r&b', 'rap', 'reggae & ska', 'reggaeton', 'rock', 'showtunes', 'singer-songwriter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite disbalanced. First, let's:\n",
    "- drop the NaN rows when count is above 1000\n",
    "- replace the NaN rows values by the median of the others when under 1000\n",
    "- combine some of the similar genres with low number of rows: international & hawai, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'd group all the international songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal.loc[(df_bal['genres'].str.contains(\"hawa\")), 'genres'] = 'international/world'\n",
    "df_bal.loc[(df_bal['genres'] == \"int'l\"), 'genres'] = 'international/world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal.loc[(df_bal['genres'] == 'showtunes'), 'genres'] = 'film/show'\n",
    "df_bal.loc[(df_bal['genres'] == 'film scores'), 'genres'] = 'film/show'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's differentiate genres that have more/less than 1000 non-NaN rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_genres_df = ['bluegrass', 'blues & blues rock', \"children's\", 'christian', 'classical', 'country', 'dance', \"dubstep & drum 'n' bass\", 'easy listening', 'electronica', 'film/show', 'folk', 'funk', 'indie', 'international/world', 'jazz', 'latin', 'nature sounds', 'oldies', 'pop', 'r&b', 'rap', 'reggae & ska', 'reggaeton', 'rock', 'singer-songwriter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_genres = []\n",
    "small_genres = []\n",
    "\n",
    "for i in new_genres_df:\n",
    "    songs_genre = df_bal[df_bal['genres'] == i]\n",
    "    songs_genre_nan = songs_genre['speechiness'].apply(checknan)\n",
    "    if len(songs_genre_nan[songs_genre_nan == False]) >= 1000:\n",
    "        large_genres.append(i)\n",
    "    else:\n",
    "        small_genres.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop NaN on large genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "for i in large_genres:\n",
    "    songs = df_bal[df_bal['genres'] == i]\n",
    "    new_songs = songs.dropna(axis=0, how='any')\n",
    "    new_df = pd.concat([new_df, new_songs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace NaN by median on small genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in small_genres:\n",
    "    songs = df_bal[df_bal['genres'] == i]\n",
    "    new_songs = songs.fillna(songs.median())\n",
    "    new_df = pd.concat([new_df, new_songs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we don't have any NaN value left. However we can see below that the dataframe is not well balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Select data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's randomize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1, random_state=101).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('genres', axis=1)\n",
    "y = new_df['genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over and undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid imbalanced data, we will also try to use a combination of over and under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_sample(X_scale, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now also have 2 new data sources: X_resampled and y_resampled on which we could test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Try classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all our classifiers we will use a pipeline (classifier + SelectKBest) as we as GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the classifiers we are going to try:\n",
    "- kNN on X_scale and y\n",
    "- kNN on X_resampled and y_resampled\n",
    "- LogReg on X_scale and y\n",
    "- LogReg on X_resampled and y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN on X_resampled and y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=101)\n",
    "knn2 = KNeighborsClassifier()\n",
    "selector2 = SelectKBest()\n",
    "steps_knn2 = [('feature_selection', selector2), ('kneighbors', knn2)]\n",
    "parameters_knn2 = dict(feature_selection__k=[5,7,10,12], kneighbors__n_neighbors=[3,5,7,10])\n",
    "pipeline_knn2 = Pipeline(steps_knn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('feature_selection', SelectKBest(k=10, score_func=<function f_classif at 0x10edcda60>)), ('kneighbors', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'feature_selection__k': [5, 7, 10, 12], 'kneighbors__n_neighbors': [3, 5, 7, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_knn2 = GridSearchCV(pipeline_knn2, param_grid=parameters_knn2)\n",
    "grid_knn2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(grid_knn2.best_estimator_)\n",
    "predictions_knn2 = grid_knn2.predict(X_test)\n",
    "#print(classification_report(y_test, predictions_knn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the resampled data gives way better results. We will keep only this kNN classifier as the first result is too low. However we can note the computation time increased significantly: so we will reduce some of the parameters later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: we should be careful about overfitting with this specific model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use GridSearchCV for LogReg but we will include more parameters as several things might be interesting: class_weight (to balance automatically the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.1, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr1 = LogisticRegression(max_iter=5000, class_weight='balanced')\n",
    "selector_lr1 = SelectKBest()\n",
    "steps_lr1 = [('feature_selection', selector_lr1), ('LogReg', lr1)]\n",
    "parameters_lr1 = dict(feature_selection__k=[5,8,12], \n",
    "                      LogReg__solver=['newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "                      LogReg__multi_class=['ovr', 'multinomial'])\n",
    "\n",
    "pipeline_lr1 = Pipeline(steps_lr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid_lr1 = GridSearchCV(pipeline_lr1, param_grid=parameters_lr1, verbose=3)\n",
    "grid_lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(grid_lr1.best_estimator_)\n",
    "predictions_lr1 = grid_lr1.predict(X_test)\n",
    "print(classification_report(y_test, predictions_lr1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing ut without the balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr2 = LogisticRegression(max_iter=5000)\n",
    "selector_lr2 = SelectKBest()\n",
    "steps_lr2 = [('feature_selection', selector_lr2), ('LogReg', lr2)]\n",
    "parameters_lr2 = dict(feature_selection__k=[5,8,12], \n",
    "                      LogReg__solver=['newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "                      LogReg__multi_class=['ovr', 'multinomial'])\n",
    "\n",
    "pipeline_lr2 = Pipeline(steps_lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid_lr2 = GridSearchCV(pipeline_lr2, param_grid=parameters_lr2, verbose=3)\n",
    "grid_lr2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(grid_lr2.best_estimator_)\n",
    "predictions_lr2 = grid_lr2.predict(X_test)\n",
    "print(classification_report(y_test, predictions_lr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: audio features formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_audio(audio_features):\n",
    "    features = np.asarray(audio_features)\n",
    "    features = features.reshape(1,-1)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    test_song_scaled = scaler.transform(features)\n",
    "    return test_song_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: go through classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre(scaled_song):\n",
    "    genre = grid_knn2.predict(scaled_song)\n",
    "    return genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(full_df2['audio_features'][15])\n",
    "# print(full_df2['genres'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = format_audio(full_df2['audio_features'][15])\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_genre(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
